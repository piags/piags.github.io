---
layout: post
category : machine learning
tags : [machine learning, normalization]
title: 《归一化》学习笔记
---

<!-- start latex解析 -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!-- end latex解析 -->


### WHAT
#### 归一化
##### 简单缩放 | min-max标准化(Min-max normalization) | 离差标准化
简单缩放通过对原始数据进行线性变换把数据映射到[0,1]之间，变换函数为：

$$x^{'}=\frac{x-\text{min}}{\text{max}-\text{min}}$$

其中min是样本中最小值，max是样本中最大值。注意在数据流场景下最大值与最小值是变化的，而且最大值与最小值非常容易受异常点影响。如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定，所以这种方法鲁棒性较差，只适合传统精确小数据场景或数值比较集中的情况。实际使用中可以用经验常量值来替代max和min，当有新数据加入时，可能导致max和min的变化，需要重新定义。

在不涉及距离度量、协方差计算、数据不符合正态分布的时候，可以使用该方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0, 255]的范围。

##### 标准差标准化 | z-score 0均值标准化(zero-mean normalization)
经过处理后的数据均值为0，标准差为1，处理方法是：

$$x^{'}=\frac{x-\mu}{\sigma}$$

其中μ是样本的均值，σ是样本的标准差，它们可以通过现有样本进行估计。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，该方法表现较好。

##### 非线性归一化
经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。

### WHY
归一化的依据非常简单，不同变量往往[量纲](http://baike.baidu.com/item/%E9%87%8F%E7%BA%B2)不同，归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。比如两个人体重差10KG，身高差0.02M，在衡量两个人的差别时体重的差距会把身高的差距完全掩盖，归一化之后就不会有这样的问题。

此外，归一化后加快了梯度下降求最优解的速度，且有可能提高精度。

#### 关于标准差标准化
标准差标准化的原理比较复杂，它表示的是原始值与均值之间差多少个标准差，是一个相对值，所以也有去除量纲的功效。同时，它还带来两个附加的好处：均值为0，标准差为1。

此外，数据归一化处理
* 加快了梯度下降求最优解的速度
* 有可能提高精度

##### 均值为0的好处
可以使数据以0为中心左右分布，而数据以0为中心左右分布会带来很多便利。比如在去中心化的数据上做SVD分解等价于在原始数据上做PCA；机器学习中很多函数如Sigmoid、Tanh、Softmax等都以0为中心左右分布（不一定对称）。

##### 标准差为1的好处
解释稍复杂。\\(M\\)维空间中节点\\(x_i\\)和节点\\(x_j\\)间的距离通常表示为
$$D(x_i,x_j) = \sum_{m=1}^{M}w_m \cdot d_m(x_i,x_j); \qquad \sum_{m=1}^{M}w_m=1$$
其中\\(d_m(x_i,x_j)\\)是两个点在属性\\(m\\)上的距离，\\(w_j\\)是该属性间的距离在总距离中的权重，注意此处设\\(w_m=1, \forall m\\)并不能实现每个属性对最后的结果贡献相同。对于给定的数据集，所有点对间距离的平均值是个定值，即

$$\bar{D} = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}D(x_i,x_j)=\sum_{m=1}^{M}w_m \cdot \bar{d_m}$$

是个常数，其中

$$ \bar{d_m} = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}d_m(x_i,x_j)$$

可见第\\(m\\)个变量对最终整体平均距离的影响是 \\(w_{m} \cdot \bar{d_m}\\)，所以设\\(w_m \sim 1/\bar{d_m}\\)可以使所有属性对全数据集平均距离的贡献相同。现在设\\(d_m\\)为欧式距离（或称为二范数）的平方，它是最常用的距离衡量方法之一，则有
$$\bar{d_m} =  \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}d_m(x_i,x_j) = 2 \cdot var_j$$
其中\\(var_j\\)是\\(Var(X_j)\\)的样本估计，也就是说每个变量的重要程度正比于这个变量在这个数据集上的方差。如果我们让每一维变量的标准差都为1（即方差都为1），每维变量在计算距离的时候重要程度相同。


### 参考
[ZHAOKV 归一化与标准化](http://www.zhaokv.com/2016/01/normalization-and-standardization.html)

[优矿 归一化(标准化)](https://uqer.io/community/share/56c3e9c6228e5b0fe6b17d95)

#### 知乎上的问题
[处理数据时不进行归一化会有什么影响？归一化的作用是什么？什么时候需要归一化？有哪些归一化的方法？](https://www.zhihu.com/question/20455227)

[在进行数据分析的时候，什么情况下需要对数据进行标准化处理？](https://www.zhihu.com/question/30038463)

[机器学习数据归一化的的方法有哪些？适合于什么样的数据？](https://www.zhihu.com/question/26546711)

[为什么 feature scaling 会使 gradient descent 的收敛更好?](https://www.zhihu.com/question/37129350)

[数据特征的归一化，是对整个矩阵还是对每一维特征？](https://www.zhihu.com/question/31186681)
